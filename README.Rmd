---
title: "# rubias --- a package for tidy genetic stock identification (GSI)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: rubias_bib.bibtex
output: 
  html_notebook:
    toc: true
  github_document:
    toc: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE, message=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "readme-figs/"
)
library(tidyverse)
```




This is an R package for perorming genetic stock identification (GSI) and
associated tasks.  Additionally, it includes a method
designed to diagnose and correct a bias recently 
documented in genetic stock identification. The bias occurs when mixture 
proportion estimates are desired for groups of populations (reporting units)
and the number of populations within each reporting unit are uneven.

In order to run C++ implementations of MCMC, rubias requires the package
Rcpp, which in turn requires an Rtools installation. After cloning into the
repository with the above dependencies installed, build & reload the package 
to view further documentation.

The script "/R-main/coalescent_sim" was used to generate coalescent simulations
for bias correction validation. This is unnecessary for testing the applicability
of our methods to any particular dataset, which should be done using
`assess_reference_loo()` and  `assess_pb_bias_correction()`.
`coalescent_sim()`  creates simulated populations using the `ms` coalescent simulation 
program, available from the Hudson lab at UChicago, and the `GSImulator`
and `ms2geno` packages, available at [https://github.com/eriqande](https://github.com/eriqande),
and so requires more dependencies than the rest of the package.


# Input Data

The functions for conducting genetic mixture analysis and for doing simulation
assessment to predict the accuracy of a set of genetic markers for genetic stock 
identification require that genetic data be input as a data frame in a
specific format:

- one row per individual
- each locus is represented by two adjacent columns, one for each allele (this package is only
  configured for diploids, at the moment). Allelic types can be expressed as any number
  or character
- missing data at a locus is expressed with NA values for each gene copy at the locus
- if one gene copy is missing in an indivividuals from a locus, then both gene copies must be missing at the locus.
- the name of the locus is taken to be the column name of the _first_ column of each pair
of locus columns.  The header on the second column is ignored.
- the data frame must have four columns of meta data for each individual:
    * `sample_type`:  a column telling whether the sample is a `reference` sample or a `mixture` sample.
    * `repunit`: the reporting unit that an individual/collection belongs to.  This is required if sample_type is 
       `reference`. And if  sample_type  is `mixture` then repunit must be `NA`.  
       This must be a character vector. Not a factor. The idea of a "reporting unit"
       is well-known amongst people doing genetic stock identfication of salmon, but might not be familiar
       elsewhere.  Briefly, a reporting unit is a group of populations (which we call "collections") that 
       are typically closely related genetically, and which will likely be aggregrated in the results of the 
       GSI exercise.
    * `collection`: for reference samples, the name of the population that the individual is from. For mixture
    samples, this is the name of the particular sample (i.e. stratum or port that is to be treated together in 
    space and time.). This must be a character, not a factor.
    * `indiv` a character vector with the ID of the fish.  These must be unique.
- When we started developing `rubias`, we intended to allow both the `repunit` and the `collection` columns to be 
either character vectors or factors.  Having them as factors might be desirable if, for example, a certain
sort order of the collections or repunits was desired.  _However_ at some point it became clear to Eric that,
given our approach to converting all the data to a C++ data structure of integers, for rapid analyis,
we would be exposing ourselves to greater opportunities for bugginess
by allowing `repunit` and `collection` to be factors.  Accordingly, they **must** be character vectors.  If they
are not, `rubias` will throw an error.  **Note**: if you do have a specific sort order for your collections or 
repunits, you can always change them into factors after analysis with `rubias`.  Additionally, you can keep 
extra columns in your original data frame (for example `repunit_f` or `collection_f`) in which the repunits or 
the collections are stored as factors.  See, for example the data file `alewife`. Or you can just keep a character
vector that has the sort order you would like, so as to use it when changing things to factors
after `rubias` analysis.  (See, for instance, `chinook_repunit_levels`.)
- The file can have any number of other meta data columns; however, _they must all occur in the data frame **before** the columns of genetic data_.
- When you pass a data frame into any of these functions, you have to tell it which column the genetic data starts
in, and it is assumed that all the columns after that one contain genetic data.
- If you are doing a mixture analyis, the data frame of mixture fish and of the reference fish must have the
same column structure, i.e., they must have exactly the same number of columns with exactly the same
column names, in the same order and of the same type.


## An example reference data file

Here are the meta data columns and the first two loci for eight individuals in the `chinook` reference data set that comes with the 
package:
```{r}
library(rubias)
head(chinook[, 1:8])
```

## An example mixture data file

Here is the same for the mixture data frame that goes along with that reference data set:
```{r}
head(chinook_mix[, 1:8])
```


# Performing a Genetic Mixture Analysis

This is done with the `infer_mixture` function.  In the example data 
`chinook_mix` our data consist of fish caught in three different fisheries, `rec1`,
`rec2`, and `rec3` as denoted in the collection column. Each of those collections is 
treated as a separate sample, getting its own mixing proportion estimate.  This is how
it is run with the default options:
```{r infer_mixture1, cache=TRUE}
mix_est <- infer_mixture(reference = chinook, 
                         mixture = chinook_mix, 
                         gen_start_col = 5)
```

The result comes back as a list of four tidy data frames:

1. `mixing_proportions`: the mixing proportions.  The column `pi` holds the estimated mixing proportion for each collection.
2. `indiv_posteriors`: this holds, for each individual, the posterior means of group membership in each
collection.  Column `PofZ` holds those values.  It also includes `n_non_miss_loci` and `n_miss_loci` which are
the number of observed loci and the number of missing loci at the individual.
3. `mix_prop_traces:` MCMC traces of the mixing proportions for each collection.
4. `bootstrapped_proportions`: This is NULL in the above example, but if we had chosen
`method = "PB"` then this would be a tibble of bootstrap-corrected reporting unit
mixing proportions.

These data frames look like this:

```{r}
lapply(mix_est, head)
```


## Aggregating collections into reporting units

This is a simple operation in the [tidyverse](http://tidyverse.org/):
```{r}
# for mixing proportions
rep_mix_ests <- mix_est$mixing_proportions %>%
  group_by(mixture_collection, repunit) %>%
  summarise(repprop = sum(pi))  # adding mixing proportions over collections in the repunit

# for individuals posteriors
rep_indiv_ests <- mix_est$indiv_posteriors %>%
  group_by(mixture_collection, indiv, repunit) %>%
  summarise(rep_pofz = sum(PofZ))
```


## Creating posterior density curves from the traces

The full MCMC output for the mixing proportions is available by default in the
field `$mix_prop_traces`.  This
can be used to obtain an estimate of the posterior density of the mixing proportions.

Here we plot kernel density estimates for the 6 most abundant repunits from the
`rec1` fishery:
```{r}
# find the top 6 most abundant:
top6 <- rep_mix_ests %>%
  filter(mixture_collection == "rec1") %>% 
  arrange(desc(repprop)) %>%
  slice(1:6)

# check how many MCMC sweeps were done:
nsweeps <- max(mix_est$mix_prop_traces$sweep)

# keep only rec1, then discard the first 200 sweeps as burn-in,
# and then aggregate over reporting units
# and then keep only the top6 from above
trace_subset <- mix_est$mix_prop_traces %>%
  filter(mixture_collection == "rec1", sweep > 200) %>%
  group_by(sweep, repunit) %>%
  summarise(repprop = sum(pi)) %>% 
  filter(repunit %in% top6$repunit)


# now we can plot those:
ggplot(trace_subset, aes(x = repprop, colour = repunit)) +
  geom_density()
```


## Bootstrap-Corrected Reporting Unit Proportions

These are obtained using `method = "PB"` in `infer_mixture()`.  When invoked, this will return the 
regular MCMC results as before, but also will population the `bootstrapped_proportions` field
of the output.  Doing so takes a little bit longer, computationally, because there is a good deal 
of simulation involved:
```{r infer_mixture_pb, cache=TRUE}
mix_est_pb <- infer_mixture(reference = chinook, 
                         mixture = chinook_mix, 
                         gen_start_col = 5,
                         method = "PB")
```

And now we can compare the estimates, showing here the 10 most prevalent
repunits, in the `rec1` fishery:
```{r, cache=TRUE}
mix_est_pb$mixing_proportions %>%
  group_by(mixture_collection, repunit) %>%
  summarise(repprop = sum(pi)) %>%
  left_join(mix_est_pb$bootstrapped_proportions) %>%
  ungroup() %>%
  filter(mixture_collection == "rec1") %>%
  arrange(desc(repprop)) %>%
  slice(1:10)
```

So, we see that the proportion of Central Valley Fall Run has been reduced in the 
bootstrap correction, and the proprtion of Central Valley Spring Run has been increased.

# Assessment of Genetic References

## Self-assigning fish from the reference

```{r}
sa_chinook <- self_assign(reference = chinook, gen_start_col = 5)
```

## Simulated mixtures using a leave-one-out type of approach

If you want to know how much accuracy you can expect given a set of genetic markers and
a grouping of populations (`collection`s) into reporting units (`repunit`s), there are two
different functions you might use:

1. `assess_reference_loo()`: This function carries out simulation of mixtures using the leave-one-out
approach of @Andersonetal2008.
2. `assess_reference_mc()`: This functions breaks the reference data set into different subsets, one of which
is used as the reference data set and the other the mixture.  It is difficult to simulate very large mixture 
samples using this method, because it is constrained by the number of fish in the reference data set.  
Additionally, there are constraints on the mixing proportions that can be simulated because of variation in the 
number of fish from each collection in the reference.

Both of the functions take two required arguments: 1) a data frame of reference genetic data, and 2) the 
number of the column in which the genetic data start.

Here we use the `chinook` data to simulate 50 mixture samples of size 200 fish 
using the default values (Dirichlet parameters of 1.5 for each reporting unit, and
Dirichlet parameters of 1.5 for each collection within a reporting unit...)
```{r, message=FALSE}
chin_sims <- assess_reference_loo(reference = chinook, 
                     gen_start_col = 5, 
                     reps = 50, 
                     mixsize = 200)
```

Here is what the output looks like:
```{r}
chin_sims
```


## Specifying mixture proportions in `assess_reference_loo()`

By default, each iteration, the proportions of fish from each reporting unit is simulated
from a Dirichlet distribution with parameter (1.5,...,1.5).  And, within each reporting unit the mixing 
proportions from different collections are
drawn from a Dirichlet distribution with parameter  (1.5,...,1.5).

The value of 1.5 for the Dirichlet parameter for reporting units can be changed using the
`alpha_repunit`. The Dirichlet parameter for collections can be set using the `alpha_collection` parameter.

Sometimes, however, more control over the composition of the simulated mixtures is desired. This is achieved
by passing a two-column _data.frame_ to either `alpha_repunit` or `alpha_collection` (or both).  If you are 
passing the data.frame in for `alpha_repunit`, the first column must be named `repunit` and it must contain 
characters specifying reporting units.  In the data.frame for `alpha_collection` the first column must be 
named `collection` and must hold strings specifying different collections.  It is an error if
a repunit or collection is specified that does not exist in the reference.  However, you do not need to 
specify a value for every reporting unit or collection.  (If they are absent, the value is assumed to be zero.)

The second column of the data frame must be one of `count`, `ppn` or `dirichlet`.  These specify, respectively,

1. the exact count of individuals to be simulated from each repunit (or collection);
2. the proportion of individuals from each repunit (or collection); or 
3. the parameters of a Dirichlet distribution from which the proportion of individuals should be simulated.
These `ppn` values will be normalized to sum to one if they do not.  As such, they can be regarded as weights.

Let's say that we want to simulate data that roughly have proportions like what we saw in the
Chinook `rec1` fishery.  We have those estimates in the variable `top6`:
```{r}
top6
```

We could, if we put those `repprop` values into a `ppn` column, simulate mixtures with 
exactly those proportions.  Or if we wanted to simulate exact numbers of fish in a sample
of `r sum(round(top6$repprop * 350))` fish, we could get those values like this:
```{r}
round(top6$repprop * 350)
```
and then put them in a `cnts` column.  

However, in this case, we want to simulate mixtures that look similar to the one we 
estimated, but have some variation.  For that we will want to supply Dirichlet random variable
parmaters in a column named `dirichlet`.  If we make the values proportional to the mixing proportions,
then, on average that is what they will be.  If the values are large, then there will be little 
variation between simulated mixtures.  And if the the values are small there will be lots of variation.  
We'll scale them so that they sum to 10---that should give some variation, but not too much.  Accordingly the
tibble that we pass in as the `alpha_repunit` parameter, which describes the variation in reporting unit
proportions we would like to simulate would look like this:
```{r}
arep <- top6 %>%
  ungroup() %>%
  mutate(dirichlet = 10 * repprop) %>%
  select(repunit, dirichlet)

arep
```

Let's do some simulations with those repunit parameters.  By default, if we don't
specify anything extra for the _collections_, they get dirichlet parameters of 1.5.
```{r, message=FALSE}
chin_sims_repu_top6 <- assess_reference_loo(reference = chinook, 
                     gen_start_col = 5, 
                     reps = 50, 
                     mixsize = 200,
                     alpha_repunit = arep)
```

Now, we can summarise it by reporting unit and then plot it for the values we are 
interested in:
```{r}
# now, call those repunits that we did not specify in arep "OTHER"
# and then sum up over reporting units
tmp <- chin_sims_repu_top6 %>%
  mutate(repunit = ifelse(repunit %in% arep$repunit, repunit, "OTHER")) %>%
  group_by(iter, repunit) %>%
  summarise(true_repprop = sum(true_pi), 
            reprop_posterior_mean = sum(post_mean_pi),
            repu_n = sum(n)) %>%
  mutate(repu_n_prop = repu_n / sum(repu_n))
  
```

```{r}
# then plot them
ggplot(tmp, aes(x = true_repprop, y = reprop_posterior_mean, colour = repunit)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~ repunit)
```

Or plot comparing to their n value.

```{r}
ggplot(tmp, aes(x = repu_n_prop, y = reprop_posterior_mean, colour = repunit)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~ repunit)
```

So, it seems to me that the bootstrap correction procedure might be 
buggy when the order of the collections is not what one expects.  There
is not way it should be so radically corrected by the bootstrap.  It seems
to me that is caused by the same issue I say earlier...

### "sub-specifying" collection proportions or dirichlet parameters


And let's see how things work if you want some control over the collection proportions
in the simulations, too:
```{r}
# here for an example, just take the top 20 collections with highest estimated proportions
top20coll_ppn <- mix_est$mixing_proportions %>% 
  filter(mixture_collection == "rec1") %>% 
  arrange(desc(pi)) %>% 
  slice(1:20) %>%
  select(-mixture_collection, -repunit) %>%
  rename(ppn = pi)

top20coll_ppn
```





# References
